{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/daniel/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_140829/2387908659.py\", line 12, in <module>\n",
      "    from dataset.Splines import Splines, ConvexHullDataset\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/daniel/Documents/Uni/practical-sose23/castellvi/3D-Castellvi-Prediction/src/dataset/__init__.py\", line 3, in <module>\n",
      "    from utils._prepare_data import DataHandler, read_config\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 992, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/daniel/Documents/Uni/practical-sose23/castellvi/3D-Castellvi-Prediction/src/utils/__init__.py\", line 2, in <module>\n",
      "    from ._prepare_data import DataHandler\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/daniel/Documents/Uni/practical-sose23/castellvi/3D-Castellvi-Prediction/src/utils/_prepare_data.py\", line 10, in <module>\n",
      "    from pqdm.processes import pqdm\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/site-packages/pqdm/processes.py\", line 6, in <module>\n",
      "    from tqdm.auto import tqdm as tqdm_auto\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/site-packages/tqdm/auto.py\", line 21, in <module>\n",
      "    from .autonotebook import tqdm as notebook_tqdm\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/home/daniel/anaconda3/envs/dev-castellvi/lib/python3.10/site-packages/tqdm/autonotebook.py\", line 19, in <module>\n",
      "    warn(WARN_NOIPYW, TqdmWarning, stacklevel=2)\n",
      "####################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] subreg is not in list of legal keys. This name 'sub-verse015_seg-subreg_subreg-castcorr_msk.nii.gz' is invalid. Legal keys are: ['sub', 'ses', 'sequ', 'acq', 'task', 'chunk', 'hemi', 'sample', 'ce', 'trc', 'stain', 'rec', 'proc', 'mod', 'recording', 'res', 'dir', 'echo', 'flip', 'inv', 'mt', 'part', 'space', 'seg', 'source', 'snapshot', 'ovl', 'run', 'label', 'split', 'den', 'desc', 'ct']. \n",
      "For use see https://bids-specification.readthedocs.io/en/stable/99-appendices/09-entities.html\n",
      "[!] Unknown format seg-ano in file sub-verse602_dir-iso_seg-ano.nii.gz\n",
      "[!] Unknown format iso-ctd in file sub-verse616_dir-iso_iso-ctd.json\n",
      "[!] \"verse549\" is not a valid key/value pair. Expected \"KEY-VALUE\" in verse549_CT-iso_seg-ano.nii.gz\n",
      "[!] \"template\" is not a valid key/value pair. Expected \"KEY-VALUE\" in sub-verse519_template_sacrum_msk.nii.gz\n",
      "[!] \"sacrum\" is not a valid key/value pair. Expected \"KEY-VALUE\" in sub-verse519_template_sacrum_msk.nii.gz\n",
      "[!] cortex is not in list of legal keys. This name 'sub-verse553_seg-subreg_cortex-missing_msk.nii.gz' is invalid. Legal keys are: ['sub', 'ses', 'sequ', 'acq', 'task', 'chunk', 'hemi', 'sample', 'ce', 'trc', 'stain', 'rec', 'proc', 'mod', 'recording', 'res', 'dir', 'echo', 'flip', 'inv', 'mt', 'part', 'space', 'seg', 'source', 'snapshot', 'ovl', 'run', 'label', 'split', 'den', 'desc', 'ct', 'subreg']. \n",
      "For use see https://bids-specification.readthedocs.io/en/stable/99-appendices/09-entities.html\n",
      "[!] Unknown format subreg in file sub-verse559_CT-sag_seg-ano_subreg.nii.gz\n",
      "[!] \"verse559\" is not a valid key/value pair. Expected \"KEY-VALUE\" in verse559_CT-sag_seg-ano_subreg.nii.gz\n",
      "[!] \"verse559\" is not a valid key/value pair. Expected \"KEY-VALUE\" in verse559_CT-sag_seg-ano.nii.gz\n",
      "[!] Unknown format ce-pv in file sub-tri087_ce-pv.nii.gz\n",
      "[!] Unknown format ce-ar in file sub-tri129_ce-ar.nii.gz\n",
      "[!] Unknown format ce-late in file sub-tri023_ce-late.nii.gz\n",
      "[!] Unknown format ce-ne in file sub-tri026_ce-ne.nii.gz\n",
      "[!] Unknown format stat in file sub-tri087_ce-ne_seg-total_stat.json\n"
     ]
    }
   ],
   "source": [
    "# Train a tiny convolutional neural network to rate the spline interpolations \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import sys\n",
    "\n",
    "WORKING_DIR = \"/home/daniel/Documents/Uni/practical-sose23/castellvi/3D-Castellvi-Prediction/\"\n",
    "\n",
    "sys.path.append(WORKING_DIR + \"src/\")\n",
    "\n",
    "from dataset.Splines import Splines, ConvexHullDataset\n",
    "from utils._prepare_data import DataHandler\n",
    "\n",
    "dataset = [WORKING_DIR  + 'data/dataset-verse19',  WORKING_DIR + 'data/dataset-verse20', WORKING_DIR + 'data/dataset-tri']\n",
    "data_types = ['rawdata',\"derivatives\"]\n",
    "image_types = [\"ct\"]\n",
    "master_list = WORKING_DIR + 'src/dataset/Castellvi_list_v3.xlsx'\n",
    "processor = DataHandler(master_list=master_list ,dataset=dataset, data_types=data_types, image_types=image_types)\n",
    "\n",
    "dataset = Splines(processor=processor, binary=True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tiny convolutional neural network to rate the spline interpolations\n",
    "class SplineRatingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SplineRatingNet, self).__init__()\n",
    "        # Input shape: (Batch_Size, 128, 3)\n",
    "        self.conv1 = nn.Conv1d(3, 16, 3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv1d(16, 32, 3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(32 * 128, 128)\n",
    "        self.act3 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Switch shape to (Batch_Size, 3, 128)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.act3(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network with the Adam optimizer and cross entropy loss\n",
    "net = SplineRatingNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define a function to train the network\n",
    "def train(net, optimizer, criterion, train_loader, test_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 10 == 9:\n",
    "                print(f'Epoch {epoch + 1}, batch {i + 1}: loss {running_loss / 10}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                inputs, labels = data\n",
    "                outputs = net(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}: accuracy {100 * correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_sampler(dataset):\n",
    "    true_labels = []\n",
    "    for index in range(len(dataset)):\n",
    "        true_labels.append(dataset[index][1])\n",
    "\n",
    "\n",
    "    true_labels = torch.tensor(true_labels)\n",
    "\n",
    "    # Count the occurrences of each true label\n",
    "    label_counts = torch.bincount(true_labels)\n",
    "\n",
    "    # Compute the inverse of the label counts to get the weights\n",
    "    per_label_weights = 1.0 / label_counts.float()\n",
    "\n",
    "    weights = torch.zeros(size = true_labels.size())\n",
    "\n",
    "    for i in range(len(label_counts)):\n",
    "        weights[true_labels == i] = per_label_weights[i] / len(label_counts)\n",
    "\n",
    "    weights = weights.tolist()\n",
    "\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(weights, 2*len(dataset), replacement = True)\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 10: loss 1.0425883084535599\n",
      "Epoch 1, batch 20: loss 0.6006335884332656\n",
      "Epoch 1, batch 30: loss 0.58604736328125\n",
      "Epoch 1, batch 40: loss 0.5605399966239929\n",
      "Epoch 1: accuracy 80.21978021978022\n",
      "Epoch 2, batch 10: loss 0.6155560255050659\n",
      "Epoch 2, batch 20: loss 0.46444404125213623\n",
      "Epoch 2, batch 30: loss 0.5027242839336395\n",
      "Epoch 2, batch 40: loss 0.5209060341119767\n",
      "Epoch 2: accuracy 70.32967032967034\n",
      "Epoch 3, batch 10: loss 0.5297072112560273\n",
      "Epoch 3, batch 20: loss 0.5244044423103332\n",
      "Epoch 3, batch 30: loss 0.46232715249061584\n",
      "Epoch 3, batch 40: loss 0.49078837037086487\n",
      "Epoch 3: accuracy 79.67032967032966\n",
      "Epoch 4, batch 10: loss 0.444505113363266\n",
      "Epoch 4, batch 20: loss 0.49460318088531496\n",
      "Epoch 4, batch 30: loss 0.5337766319513321\n",
      "Epoch 4, batch 40: loss 0.4740148395299911\n",
      "Epoch 4: accuracy 78.02197802197803\n",
      "Epoch 5, batch 10: loss 0.5190467864274979\n",
      "Epoch 5, batch 20: loss 0.49424166679382325\n",
      "Epoch 5, batch 30: loss 0.4706708490848541\n",
      "Epoch 5, batch 40: loss 0.4806658238172531\n",
      "Epoch 5: accuracy 71.42857142857143\n",
      "Epoch 6, batch 10: loss 0.5274169951677322\n",
      "Epoch 6, batch 20: loss 0.5451883167028427\n",
      "Epoch 6, batch 30: loss 0.4475860565900803\n",
      "Epoch 6, batch 40: loss 0.4435273379087448\n",
      "Epoch 6: accuracy 77.47252747252747\n",
      "Epoch 7, batch 10: loss 0.4639600783586502\n",
      "Epoch 7, batch 20: loss 0.4612300366163254\n",
      "Epoch 7, batch 30: loss 0.5503124862909317\n",
      "Epoch 7, batch 40: loss 0.4847439408302307\n",
      "Epoch 7: accuracy 68.68131868131869\n",
      "Epoch 8, batch 10: loss 0.44386202096939087\n",
      "Epoch 8, batch 20: loss 0.5183829218149185\n",
      "Epoch 8, batch 30: loss 0.4735152989625931\n",
      "Epoch 8, batch 40: loss 0.5320188492536545\n",
      "Epoch 8: accuracy 66.48351648351648\n",
      "Epoch 9, batch 10: loss 0.4733941376209259\n",
      "Epoch 9, batch 20: loss 0.45702569782733915\n",
      "Epoch 9, batch 30: loss 0.5062628298997879\n",
      "Epoch 9, batch 40: loss 0.5152454882860183\n",
      "Epoch 9: accuracy 78.02197802197803\n",
      "Epoch 10, batch 10: loss 0.5269837856292725\n",
      "Epoch 10, batch 20: loss 0.5005603522062302\n",
      "Epoch 10, batch 30: loss 0.5074195623397827\n",
      "Epoch 10, batch 40: loss 0.4937226176261902\n",
      "Epoch 10: accuracy 73.62637362637362\n"
     ]
    }
   ],
   "source": [
    "# Define data loaders with weighted random sampling\n",
    "sampler = get_weighted_sampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train the network\n",
    "train(net, optimizer, criterion, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[87 57]\n",
      " [25 13]]\n",
      "F1 Score:  0.46021412037037035\n",
      "MCC:  -0.04488655290372549\n",
      "Cohens Kappa:  -0.041015625\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.60      0.68       144\n",
      "           1       0.19      0.34      0.24        38\n",
      "\n",
      "    accuracy                           0.55       182\n",
      "   macro avg       0.48      0.47      0.46       182\n",
      "weighted avg       0.65      0.55      0.59       182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate confusion matrix, F1 score for test dataset\n",
    "from sklearn.metrics import confusion_matrix, f1_score, matthews_corrcoef, cohen_kappa_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions for test dataset\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data in torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.append(predicted)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "y_pred = np.concatenate(predictions)\n",
    "y_true = np.array([label for _, label in test_dataset])\n",
    "\n",
    "# Print metrics and report\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"F1 Score: \", f1_score(y_true, y_pred, average='macro'))\n",
    "print(\"MCC: \", matthews_corrcoef(y_true, y_pred))\n",
    "print(\"Cohens Kappa: \", cohen_kappa_score(y_true, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not convincing at all. Let's see if the convex hull is more helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ConvexHullDataset(processor=processor, binary = True)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tiny convolutional neural network to rate the convex vertices\n",
    "class ConvexHullRatingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvexHullRatingNet, self).__init__()\n",
    "        # Input shape: (Batch_Size, 256, 6)\n",
    "        self.conv1 = nn.Conv1d(6, 16, 3, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv1d(16, 32, 3, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.out = nn.Linear(32 * 256, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Switch shape to (Batch_Size, 6, 256)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.act1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.act2(x)\n",
    "\n",
    "        x = self.flat(x)\n",
    "\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 10: loss 4.9730241894721985\n",
      "Epoch 1, batch 20: loss 1.5269003033638\n",
      "Epoch 1, batch 30: loss 0.8526908785104752\n",
      "Epoch 1, batch 40: loss 0.46973847299814225\n",
      "Epoch 1: accuracy 68.13186813186813\n",
      "Epoch 2, batch 10: loss 0.2773231193423271\n",
      "Epoch 2, batch 20: loss 0.20079188272356988\n",
      "Epoch 2, batch 30: loss 0.20724923387169839\n",
      "Epoch 2, batch 40: loss 0.1413572683930397\n",
      "Epoch 2: accuracy 82.96703296703296\n",
      "Epoch 3, batch 10: loss 0.10857817120850086\n",
      "Epoch 3, batch 20: loss 0.06848738603293895\n",
      "Epoch 3, batch 30: loss 0.0799126025289297\n",
      "Epoch 3, batch 40: loss 0.0567275395616889\n",
      "Epoch 3: accuracy 87.91208791208791\n",
      "Epoch 4, batch 10: loss 0.0410504998639226\n",
      "Epoch 4, batch 20: loss 0.03731065336614847\n",
      "Epoch 4, batch 30: loss 0.029471299797296523\n",
      "Epoch 4, batch 40: loss 0.019559367652982475\n",
      "Epoch 4: accuracy 87.36263736263736\n",
      "Epoch 5, batch 10: loss 0.02080434951931238\n",
      "Epoch 5, batch 20: loss 0.014101482182741164\n",
      "Epoch 5, batch 30: loss 0.010628015641123057\n",
      "Epoch 5, batch 40: loss 0.012435479322448373\n",
      "Epoch 5: accuracy 86.26373626373626\n",
      "Epoch 6, batch 10: loss 0.008862465899437667\n",
      "Epoch 6, batch 20: loss 0.010863331053406\n",
      "Epoch 6, batch 30: loss 0.007616473129019141\n",
      "Epoch 6, batch 40: loss 0.015010929014533759\n",
      "Epoch 6: accuracy 82.96703296703296\n",
      "Epoch 7, batch 10: loss 0.016855355142615734\n",
      "Epoch 7, batch 20: loss 0.007404653262346983\n",
      "Epoch 7, batch 30: loss 0.005875599570572376\n",
      "Epoch 7, batch 40: loss 0.004935574205592275\n",
      "Epoch 7: accuracy 84.06593406593407\n",
      "Epoch 8, batch 10: loss 0.006548587628640234\n",
      "Epoch 8, batch 20: loss 0.003517406270839274\n",
      "Epoch 8, batch 30: loss 0.003722209483385086\n",
      "Epoch 8, batch 40: loss 0.0031809825683012604\n",
      "Epoch 8: accuracy 84.06593406593407\n",
      "Epoch 9, batch 10: loss 0.0026910337852314114\n",
      "Epoch 9, batch 20: loss 0.002328850096091628\n",
      "Epoch 9, batch 30: loss 0.0021892488934099675\n",
      "Epoch 9, batch 40: loss 0.0018075763829983771\n",
      "Epoch 9: accuracy 84.06593406593407\n",
      "Epoch 10, batch 10: loss 0.0015713009575847535\n",
      "Epoch 10, batch 20: loss 0.0015240863314829767\n",
      "Epoch 10, batch 30: loss 0.0016040872083976864\n",
      "Epoch 10, batch 40: loss 0.001516541780438274\n",
      "Epoch 10: accuracy 84.06593406593407\n"
     ]
    }
   ],
   "source": [
    "#Train the network with the Adam optimizer and cross entropy loss\n",
    "net = ConvexHullRatingNet()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "sampler = get_weighted_sampler(train_dataset)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, sampler=sampler)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "train(net, optimizer, criterion, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      " [[123  21]\n",
      " [ 34   4]]\n",
      "F1 Score:  0.4721299372462164\n",
      "MCC:  -0.047902711776139334\n",
      "Cohens Kappa:  -0.04641438427764988\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.85      0.82       144\n",
      "           1       0.16      0.11      0.13        38\n",
      "\n",
      "    accuracy                           0.70       182\n",
      "   macro avg       0.47      0.48      0.47       182\n",
      "weighted avg       0.65      0.70      0.67       182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get predictions for test dataset\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for data in torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True):\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predictions.append(predicted)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "y_pred = np.concatenate(predictions)\n",
    "y_true = np.array([label for _, label in test_dataset])\n",
    "\n",
    "# Print metrics and report\n",
    "print(\"Confusion Matrix: \\n\", confusion_matrix(y_true, y_pred))\n",
    "print(\"F1 Score: \", f1_score(y_true, y_pred, average='macro'))\n",
    "print(\"MCC: \", matthews_corrcoef(y_true, y_pred))\n",
    "print(\"Cohens Kappa: \", cohen_kappa_score(y_true, y_pred))\n",
    "print(\"Classification Report: \\n\", classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-castellvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
